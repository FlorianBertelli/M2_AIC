{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT0dXdRMf_LG"
   },
   "source": [
    "# Practical classes\n",
    "\n",
    "All exercices will be in Python. It is important that you keep track of exercices and structure you code correctly (e.g. create funcions that you can re-use later)\n",
    "\n",
    "We will use Jupyter notebooks (formerly known as IPython). You can read the following courses for help:\n",
    "* Python and numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "* Jupyter / IPython : http://cs231n.github.io/ipython-tutorial/\n",
    "\n",
    "To run this notebook:\n",
    "* create a directory somewhere on your filesystem\n",
    "* download the .ipynb from the course website: http://teaching.caio-corro.fr/2019-2020/TC4/\n",
    "* move the .ipynb into the directory\n",
    "* from a terminal:\n",
    "    * cd /directory/path\n",
    "    * jupyter notebook\n",
    "    \n",
    "Each group must send me their work by e-mail (one mail per group):\n",
    "* complete the code with comments\n",
    "* quick answer to questions\n",
    "\n",
    "If you don't want to use the notebook, send me the python code + a PDF with plots and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# NLTK will be used to load the data\n",
    "import nltk\n",
    "\n",
    "# matplotlib will be used to display plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# in this exercise, numpy is only used to compute plotting data\n",
    "import numpy as np\n",
    "\n",
    "# So we can display plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT0dXdRMf_LG"
   },
   "source": [
    "# Part-of-speech tagging\n",
    "\n",
    "The goal of this exercise is to build a (really) simple **part-of-speech** (POS) tagger: given a sentence, predict the grammatical category of each word (verb, noun, etc). To this end, we introduce the following probability model: let *X* and *Y* be two random variables where *X* is an observed word and *Y* an observed tag. At test time, we will predict the most probable tag for a given word.\n",
    "\n",
    "# Data: the Brown corpus\n",
    "\n",
    "In this exercise we are going to look at the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) which is a collection of texts from different domains. It has been manually annotated with POS tags.\n",
    "\n",
    "Contrary to popular belief, there is not a single way to analyze natural language texts. One usually rely on a \"framework\" or annotation scheme that exposes desired characteristics (see for example [Rambow, 2010](https://www.aclweb.org/anthology/N10-1049/)). In this exercise, we will us the *universal tagset* [Petrov et al, 2011](https://arxiv.org/abs/1104.2086), a simple annotation scheme that was originally proposed for multilingual natural language processing (see the [universal dependendency page about morphology](https://universaldependencies.org/u/overview/morphology.html) for more information).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\berte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\berte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Brown corpus and the annotation scheme\n",
    "# This cell can be run only once on your computer\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1473
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 620,
     "status": "error",
     "timestamp": 1539681381541,
     "user": {
      "displayName": "Xihui Wang",
      "photoUrl": "",
      "userId": "09445164112052208872"
     },
     "user_tz": -120
    },
    "id": "mLdK0SLNf_LI",
    "outputId": "4c7f4534-0c3c-4c52-cf52-3b5859caf7a3"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# the argument tagset='universal' will map the original annotation to the universal tagset\n",
    "brown_data = list(nltk.corpus.brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows:\n",
    "* brown_data is a list of sentences\n",
    "* a sentence brown_data[i] is a list of tokens\n",
    "* each token brown_data[i][j] is a tuple of strings (word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "error",
     "timestamp": 1539681466176,
     "user": {
      "displayName": "Xihui Wang",
      "photoUrl": "",
      "userId": "09445164112052208872"
     },
     "user_tz": -120
    },
    "id": "OUWlaU-nf_Li",
    "outputId": "9ed44a04-79fa-40c1-e4bf-1dfb0c41e7af"
   },
   "outputs": [],
   "source": [
    "print(type(brown_data), type(brown_data[0]), type(brown_data[0][0]))\n",
    "print(\"Number of sentences: %i\" % len(brown_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing\n",
    "\n",
    "Split the dataset into two disjoint parts:\n",
    "* *train data* that will be used to learn the probability distribution p(y | x)\n",
    "* *test data* that will be used to evaluate the model\n",
    "We use 90% of the data as train data and 10% as test data.\n",
    "\n",
    "Note that the Brown corpus is a concatenation of texts from different domains. Therefore, we need to distribute the train/test split accross the corpus: you cannot just take the first 10% of sentences as test data and the remaining ones as train data.\n",
    "\n",
    "How can you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3ESvRevmd3B"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the train data: %i\" % len(train_data))\n",
    "print(\"Size of the test data: %i\" % len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data visualization\n",
    "\n",
    "We now analyze the data. First we will visualize the distribution of POS tags in the train and test sets. They should be roughly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - data: a dataset\n",
    "# Output:\n",
    "# a dictionnary where each key is a POS tag and the associated value is its frequency in the dataset\n",
    "def compute_pos_distribution(data):\n",
    "    # TODO\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_distribution = compute_pos_distribution(test_data)\n",
    "train_pos_distribution = compute_pos_distribution(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of POS tags in the train and dev sets.\n",
    "# This code is an adaptation of:\n",
    "# https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "\n",
    "# built the list of tags (categories)\n",
    "labels = list(train_pos_distribution.keys())\n",
    "\n",
    "# built the lists of category distributions for the 2 datasets\n",
    "train_probs = [train_pos_distribution[k] for k in labels]\n",
    "test_probs = [test_pos_distribution[k] for k in labels]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train_probs, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, test_probs, width, label='Dev')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Distribution')\n",
    "ax.set_title('Distribution of POS tags')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - data: a dataset\n",
    "# Output:\n",
    "# a dictionnary where each key is a word and the associated value is the number of occurences of this word in the dataset\n",
    "def compute_word_counts(data):\n",
    "    # TODO\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_counts = compute_word_counts(train_data)\n",
    "test_word_counts = compute_word_counts(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - data: a dataset\n",
    "# - words: a set of words\n",
    "# Output:\n",
    "# the percentage of words in the dataset that are in the input set words\n",
    "def compute_cover(dataset, words):\n",
    "    # TODO\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to display a plot that shows the percentage of the train/test datasets that are covered with the N most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "y_train = list()\n",
    "y_dev = list()\n",
    "\n",
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the data. What can you deduce from this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(14, 10))\n",
    "plt.plot(x, y_train)\n",
    "plt.plot(x, y_dev)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the tagger\n",
    "\n",
    "We now train a really simple discriminative POS tagger. We will learn the distribution p(y | x) where y is a tag and x is a word.\n",
    "\n",
    "Some words in the test data may not appear in the training data. In order to be robust to unknown words, we will learn the distribution p(y | x) for words that appear 10 or more times in the training data. Words that appears less than 10 times must be mapped to the \"\\*\\*UNK\\*\\*\" word. p(y | \\*\\*UNK\\*\\*) will be used to classify words that did not appear in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsgBFrgcf_Lm"
   },
   "outputs": [],
   "source": [
    "# pos_distribution_per_word should a dict of dicts:\n",
    "# keys are words (including **UNK**) and values are dicts containing the POS tag distributions.\n",
    "#\n",
    "# p(y | x) is then pos_distribution_per_word[x][y]\n",
    "\n",
    "pos_distribution_per_word = \n",
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is percentage of words that have a single tag associated with it, i.e. words x such that there exists a tag for which p(yÂ | x) = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is percentage of words have a tag with a probability greater the 90%, i.e. words x such that there exists a tag for which p(y | x) > 0.9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "To evaluate the model, we perform maximum a posteriori classification on the test data:\n",
    "1. for each word in the test data, predict the most probable tag under the p(y | x) distribution\n",
    "2. the accuracy the the number of correctly predicted tags divided by the number of words in the test data\n",
    "\n",
    "What is the accuracy of the model on test data?\n",
    "\n",
    "How does it performs comparatively to a model that would have performed a priori classification? (you can look at the POS distribution plot above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TC4-tp1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
