{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - lab exercise 1\n",
    "\n",
    "In this first lab exercise we will implement a simple bag-of-word classifier, i.e. a classifier that ignores the sequential structure of the sentence. The goal is to predict if a sentence is a positive or negative review of a movie. We will use a dataset constructed from IMDB.\n",
    "\n",
    "1. Load and clean the data\n",
    "2. Preprocess the data for the NN\n",
    "3. Module definition\n",
    "4. Train the network!\n",
    "\n",
    "We will implement this model with Pytorch, the most popular deep learning framework for Natural Language Processing. You can use the following links for help:\n",
    "* turorials: http://pytorch.org/tutorials/\n",
    "* documentation: http://pytorch.org/docs/master/ \n",
    "\n",
    "**Hard deadline**: December 3, 2019\n",
    "\n",
    "\n",
    "## Report\n",
    "\n",
    "On top of this notebook, you must submit a report explaining your neural network, it's training method and report+analyse results, for example with different hyper-parameters (number of hidden layers, different embedding size, different hidden representation size, w or w/o dropout...you choose what you want to explore!).\n",
    "You can also report results with different training dataset sizes (how does the number of training instances impact dev/test results? => *WARNING* obviously, here you change the training set size but you freeze the dev/test sets).\n",
    "\n",
    "Training with the full dataset may be slow depending on your computer.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data can be download here: http://teaching.caio-corro.fr/2019-2020/OPT7/imdb.zip\n",
    "\n",
    "There are two files: one with positive reviews (imdb.pos) and one with negative reviews (imdb.neg). Each file contains 300000 reviews, one per line.\n",
    "\n",
    "\n",
    "The following functions can be used to load and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(f.readline()).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell load the first 5000 sentences in each review set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile = 'imdb.pos'  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = 'imdb.neg' # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between train / dev / test, for example by creating lists txt_train, label_train, txt_dev, ... You should take care to keep a 50/50 ratio between positive and negative instances in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = postxt + negtxt\n",
    "label = np.concatenate((np.ones(len(postxt)),np.zeros(len(negtxt))),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'in', 'the', 'back', 'of', 'the', 'van', '!']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(data[10])\n",
    "print(label[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.1, random_state=1 ,stratify=label)\n",
    "X_train,X_dev,y_train,y_dev = train_test_split(X_train , y_train , test_size=0.1, random_state = 1 , stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laughton', 'enjoyable', 'in', 'this', 'low', 'budget', 'pirate', 'saga']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to Pytorch tensors\n",
    "\n",
    "We will first convert data to Pytorch tensors so they can be used in a neural network.\n",
    "To do that, you must first create a dictionnary that will map words to integers.\n",
    "Add to the dictionnary only words that are in the training set (be sure to understand why we do that!).\n",
    "\n",
    "Then, you can convert the data to tensors:\n",
    "- use tensors of longs: both the sentence and the label will be represented as integers, not floats!\n",
    "- these tensors do not require a gradient\n",
    "\n",
    "A tensor representing a sentence is composed of the integer representation of each word, e.g. [10, 256, 3, 4].\n",
    "Note that some words in the dev and test sets may not be in the dictionnary! (i.e. unknown words)\n",
    "You can just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict:\n",
    "    def __init__(self, words, unk=None):\n",
    "        self._unk = unk\n",
    "        self._word_to_id = dict()\n",
    "        self._id_to_word = list()\n",
    "\n",
    "        if unk in words:\n",
    "            raise RuntimeError(\"UNK word exists in vocabulary\")\n",
    "\n",
    "        if unk is not None:\n",
    "            self.unk_index = self._add_word(unk)\n",
    "\n",
    "        for word in words:\n",
    "            self._add_word(word)\n",
    "\n",
    "    # for internal use only!\n",
    "    def _add_word(self, word):\n",
    "        if word not in self._word_to_id:\n",
    "            id = len(self._id_to_word)\n",
    "            self._word_to_id[word] = id\n",
    "            self._id_to_word.append(word)\n",
    "            return id\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def str_to_id(self, word):\n",
    "        if self._unk is not None:\n",
    "            return self._word_to_id.get(word, self.unk_index)\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def id_to_str(self, id):\n",
    "        return self._id_to_word[id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def has_unk(self):\n",
    "        return self._unk is not None\n",
    "    \n",
    "    def unk(self):\n",
    "        return self.unk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict where each word is associated to a number\n",
    "token_index = {}\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "#Don't forget to add the unk = True because some words maybe in the dev or test but not in the train\n",
    "token_index = Dict(token_index, unk=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7032\n"
     ]
    }
   ],
   "source": [
    "print(len(token_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network definition\n",
    "\n",
    "The neural network should be defined as follows:\n",
    "- it takes as input a tensor that is a sequence of integers indexing word embeddings\n",
    "- it retrieves the word embeddings from an embedding table\n",
    "- it constructs the \"input\" of the MLP by summing over all embeddings√Ç (i.e. bag-of-word model)\n",
    "- it build a hidden represention using a MLP (1 layer? 2 layers? experiment! but maybe first try wihout any hidden layer...)\n",
    "- it project the hidden representation to the output space: it is a binary classification task, so the output space is a scalar where a negative (resp. positive) value means the review is negative (resp. positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, list_size, feat_size, pool_type):\n",
    "        super(Conv_classifier, self).__init__()\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.emb_db = nn.Embedding(num_embeddings = vocab_size ,embedding_dim=embedding_dim)\n",
    "        #Conv layers\n",
    "        self.convs = torch.nn.ModuleList([torch.nn.Linear(size*embedding_dim,feat_size) for size in list_size])\n",
    "        #Last layer\n",
    "        self.l1 = nn.Linear(feat_size, 1)\n",
    "        \n",
    "        #Various parameters you need\n",
    "        self.list_size = list_size\n",
    "        self.pool_type = pool_type\n",
    "        #inits\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs ):\n",
    "        idx = 0\n",
    "        for size in self.list_size:\n",
    "            if(inputs.shape[0]<size):\n",
    "                pad = torch.tensor([0 for i in range(size-inputs.shape[0])])\n",
    "                inputs = torch.cat((inputs, pad), 0)\n",
    "\n",
    "            inputs = self.emb_db(inputs)\n",
    "            cat_inputs= []\n",
    "            h = torch.Tensor(inputs.shape[0], inputs.shape[1]*size)\n",
    "\n",
    "            for i in range(len(inputs)-size+1):\n",
    "                tab = [inputs[j] for j in range(i, size+i)]\n",
    "                cat = torch.cat(tab, 0)\n",
    "                cat_inputs.append(self.convs[idx](cat))\n",
    "\n",
    "            h = torch.stack(cat_inputs)\n",
    "            \n",
    "            if (self.pool_type == \"max\"):\n",
    "#             #MAX\n",
    "                h, _ = torch.max(h, 0)\n",
    "            if (self.pool_type == \"mean\"):\n",
    "            #MEAN\n",
    "                h = torch.mean(h,0)\n",
    "        out = self.l1(h)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Create a loss function builder.\n",
    "\n",
    "- Pytorch loss functions are documented here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "- In our case, we are interested in *BCELoss* and *BCEWithLogitsLoss*. Read their documentation and choose the one that fits with your network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Write your training loop!\n",
    "\n",
    "- parameterizable number of epochs\n",
    "- at each epoch, print the mean loss and the dev accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , epochs, lr ,X_train ,y_train , X_dev , y_dev , optimizer =\"SGD\"):\n",
    "    \n",
    "    #defining various types of optimizer\n",
    "    if (optimizer==\"SGD\"):\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr =lr, weight_decay=1e-4)   \n",
    "    if (optimizer== \"AdaDelta\"):\n",
    "        optimizer = torch.optim.Adadelta(params=model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "        \n",
    "        \n",
    "    #variables to see the progression\n",
    "    mean_loss_train = []\n",
    "    mean_loss_dev = []\n",
    "    acc_train = []\n",
    "    acc_dev = []\n",
    "    \n",
    "    \n",
    "    #passe le modele en mode training\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        a = 0\n",
    "        #defining parameters to print the evolution of the NN\n",
    "        accuracy = 0\n",
    "        mean_loss = 0\n",
    "        \n",
    "        #shuffle to reduce overfitting\n",
    "#         c = list(zip(X_train, y_train))\n",
    "#         random.shuffle(c)\n",
    "#         X_train, y_train = zip(*c)\n",
    "        \n",
    "        for sent,label in zip(X_train , y_train):\n",
    "#             print('LEN', len(sent))\n",
    "#             if (len(sent)<3):\n",
    "#                  a+=1\n",
    "#                  continue\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #creating the tensors for inputs\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "\n",
    "            label = torch.Tensor([label])   \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "           # print(pred)\n",
    "            output = loss(pred , label)\n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            #backward and updates\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        mean_loss_train.append(mean_loss/len(X_train))\n",
    "        acc_train.append(accuracy/len(X_train))\n",
    "        print(\"EPOCHS\" , str(i+1) ,\"On training : Mean Loss :\" , mean_loss/len(X_train) , \"Accuracy :\" ,accuracy/len(X_train))\n",
    "\n",
    "        \n",
    "        mean_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        ###testing on dev data#########\n",
    "        for sent,label in zip(X_dev,y_dev):\n",
    "            if (len(sent)<3):\n",
    "                continue\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "            label = torch.Tensor([label])\n",
    "            \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "            output = loss(pred , label)\n",
    "            \n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            \n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        print(\"EPOCH\" , str(i+1) ,\"On Dev  :Mean Loss :\" , mean_loss/len(X_dev) , \"Accuracy :\" ,accuracy/len(X_dev))\n",
    "        print()\n",
    "\n",
    "        mean_loss_dev.append(mean_loss/len(X_dev))\n",
    "        acc_dev.append(accuracy/len(X_dev))\n",
    "          \n",
    "            \n",
    "            \n",
    "    #Plotting loss accuracy for  test and train\n",
    "    plt.plot(acc_train , label='Train')\n",
    "    plt.plot(acc_dev, label=\"Dev\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(mean_loss_train , label='Train')\n",
    "    plt.plot(mean_loss_dev, label=\"Dev\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a5200b739fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Conv_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "m = Conv_classifier(len(token_index), 5, [2], 5, pool_type=\"max\")\n",
    "train(m , 30 , lr=0.1 , X_train =X_train ,y_train = y_train , X_dev = X_dev , y_dev = y_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE TRAINED EMBEDDING LAYER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we load the files containing the pre-trained embeddings (from glove data set), and we save them with pickle for futur uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "glove_path = '.'\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27279  ,  0.77515  , -0.10181  , -0.9166   ,  0.90477  ,\n",
       "       -0.070501 , -0.47569  ,  0.44608  ,  0.1697   ,  0.072352 ,\n",
       "       -0.16306  ,  0.86852  , -0.76634  , -0.016103 ,  0.78492  ,\n",
       "        0.2952   , -0.74859  ,  0.2099   ,  0.65537  , -0.62334  ,\n",
       "       -0.43711  ,  1.1854   ,  0.47519  ,  0.0093866,  1.1377   ,\n",
       "       -2.4394   , -1.5619   ,  0.49001  ,  1.0985   , -0.97371  ,\n",
       "        3.4628   ,  1.0408   , -0.65138  ,  0.57189  , -0.12523  ,\n",
       "        0.26705  ,  0.16373  ,  0.41105  ,  0.7509   , -0.77923  ,\n",
       "        0.03638  , -0.28609  , -0.72365  ,  0.63511  ,  0.089441 ,\n",
       "       -0.30133  ,  0.36518  , -0.73367  ,  0.040383 ,  0.26657  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['my']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained =[item for sublist in X_train for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(token_index)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "for i in range (len(token_index)):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[token_index.id_to_str(i)]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(50, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7032, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv_classifier_pretrained(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, list_size, feat_size , pool_type):\n",
    "        super(Conv_classifier_pretrained, self).__init__()\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.emb_db, _, embedding_dim = create_emb_layer(torch.tensor(weights_matrix), False)\n",
    "        #Conv layers\n",
    "        self.convs = torch.nn.ModuleList([torch.nn.Linear(size*embedding_dim,feat_size) for size in list_size])\n",
    "        #Last layer\n",
    "        self.l1 = nn.Linear(feat_size, 1)\n",
    "        \n",
    "        #Various parameters you need\n",
    "        self.list_size = list_size\n",
    "        self.pool_type = pool_type\n",
    "        #inits\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        idx = 0\n",
    "        for size in self.list_size:\n",
    "            if(inputs.shape[0]<size):\n",
    "                pad = torch.tensor([0 for i in range(size-inputs.shape[0])])\n",
    "                inputs = torch.cat((inputs, pad), 0)\n",
    "\n",
    "            inputs = self.emb_db(torch.tensor(inputs))\n",
    "            cat_inputs= []\n",
    "            h = torch.Tensor(inputs.shape[0], inputs.shape[1]*size)\n",
    "\n",
    "            for i in range(len(inputs)-size+1):\n",
    "                tab = [inputs[j] for j in range(i, size+i)]\n",
    "                cat = torch.cat(tab, 0)\n",
    "                cat_inputs.append(self.convs[idx](cat))\n",
    "\n",
    "            h = torch.stack(cat_inputs)\n",
    "   \n",
    "            if (self.pool_type == \"max\"):\n",
    "#             #MAX\n",
    "                h, _ = torch.max(h, 0)\n",
    "            if (self.pool_type == \"mean\"):\n",
    "            #MEAN\n",
    "                h = torch.mean(h,0)\n",
    "        out = self.l1(h)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pretrained(model , epochs, lr ,X_train ,y_train , X_dev , y_dev , optimizer =\"SGD\"):\n",
    "    \n",
    "    #defining various types of optimizer\n",
    "    if (optimizer==\"SGD\"):\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr =lr, weight_decay=1e-4)   \n",
    "    if (optimizer== \"AdaDelta\"):\n",
    "        optimizer = torch.optim.Adadelta(params=model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "        \n",
    "        \n",
    "    #variables to see the progression\n",
    "    mean_loss_train = []\n",
    "    mean_loss_dev = []\n",
    "    acc_train = []\n",
    "    acc_dev = []\n",
    "    \n",
    "    \n",
    "    #passe le modele en mode training\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        a = 0\n",
    "        #defining parameters to print the evolution of the NN\n",
    "        accuracy = 0\n",
    "        mean_loss = 0\n",
    "        \n",
    "        #shuffle to reduce overfitting\n",
    "#         c = list(zip(X_train, y_train))\n",
    "#         random.shuffle(c)\n",
    "#         X_train, y_train = zip(*c)\n",
    "        \n",
    "        for sent,label in zip(X_train , y_train):\n",
    "#             print('LEN', len(sent))\n",
    "#             if (len(sent)<3):\n",
    "#                  a+=1\n",
    "#                  continue\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #creating the tensors for inputs\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "\n",
    "            label = torch.Tensor([label])   \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "           # print(pred)\n",
    "            output = loss(pred , label)\n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            #backward and updates\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        mean_loss_train.append(mean_loss/len(X_train))\n",
    "        acc_train.append(accuracy/len(X_train))\n",
    "        print(\"EPOCHS\" , str(i+1) ,\"On training : Mean Loss :\" , mean_loss/len(X_train) , \"Accuracy :\" ,accuracy/len(X_train))\n",
    "\n",
    "        \n",
    "        mean_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        ###testing on dev data#########\n",
    "        for sent,label in zip(X_dev,y_dev):\n",
    "            if (len(sent)<3):\n",
    "                continue\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "            label = torch.Tensor([label])\n",
    "            \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "            output = loss(pred , label)\n",
    "            \n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            \n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        print(\"EPOCH\" , str(i+1) ,\"On Dev  :Mean Loss :\" , mean_loss/len(X_dev) , \"Accuracy :\" ,accuracy/len(X_dev))\n",
    "        print()\n",
    "\n",
    "        mean_loss_dev.append(mean_loss/len(X_dev))\n",
    "        acc_dev.append(accuracy/len(X_dev))\n",
    "          \n",
    "            \n",
    "            \n",
    "    #Plotting loss accuracy for  test and train\n",
    "    plt.plot(acc_train , label='Train')\n",
    "    plt.plot(acc_dev, label=\"Dev\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(mean_loss_train , label='Train')\n",
    "    plt.plot(mean_loss_dev, label=\"Dev\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berte\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS 1 On training : Mean Loss : 0.544889495107489 Accuracy : 0.7141975308641976\n",
      "EPOCH 1 On Dev  :Mean Loss : 0.49716020020759766 Accuracy : 0.6111111111111112\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-a3fd0fa6fc23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv_classifier_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-ff32893712bb>\u001b[0m in \u001b[0;36mtrain_pretrained\u001b[1;34m(model, epochs, lr, X_train, y_train, X_dev, y_dev, optimizer)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m#prediction and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m            \u001b[1;31m# print(pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-4cce6040401a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mtab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mcat_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m2 = Conv_classifier_pretrained(len(token_index), weights_matrix, [3], 5, pool_type=\"max\")\n",
    "train_pretrained(m2 , 30 , lr=0.01 , X_train =X_train ,y_train = y_train , X_dev = X_dev , y_dev = y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
