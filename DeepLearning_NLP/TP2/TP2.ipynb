{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - lab exercise 1\n",
    "\n",
    "In this first lab exercise we will implement a simple bag-of-word classifier, i.e. a classifier that ignores the sequential structure of the sentence. The goal is to predict if a sentence is a positive or negative review of a movie. We will use a dataset constructed from IMDB.\n",
    "\n",
    "1. Load and clean the data\n",
    "2. Preprocess the data for the NN\n",
    "3. Module definition\n",
    "4. Train the network!\n",
    "\n",
    "We will implement this model with Pytorch, the most popular deep learning framework for Natural Language Processing. You can use the following links for help:\n",
    "* turorials: http://pytorch.org/tutorials/\n",
    "* documentation: http://pytorch.org/docs/master/ \n",
    "\n",
    "**Hard deadline**: December 3, 2019\n",
    "\n",
    "\n",
    "## Report\n",
    "\n",
    "On top of this notebook, you must submit a report explaining your neural network, it's training method and report+analyse results, for example with different hyper-parameters (number of hidden layers, different embedding size, different hidden representation size, w or w/o dropout...you choose what you want to explore!).\n",
    "You can also report results with different training dataset sizes (how does the number of training instances impact dev/test results? => *WARNING* obviously, here you change the training set size but you freeze the dev/test sets).\n",
    "\n",
    "Training with the full dataset may be slow depending on your computer.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data can be download here: http://teaching.caio-corro.fr/2019-2020/OPT7/imdb.zip\n",
    "\n",
    "There are two files: one with positive reviews (imdb.pos) and one with negative reviews (imdb.neg). Each file contains 300000 reviews, one per line.\n",
    "\n",
    "\n",
    "The following functions can be used to load and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(f.readline()).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell load the first 5000 sentences in each review set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile = 'imdb.pos'  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = 'imdb.neg' # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between train / dev / test, for example by creating lists txt_train, label_train, txt_dev, ... You should take care to keep a 50/50 ratio between positive and negative instances in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = postxt + negtxt\n",
    "label = np.concatenate((np.ones(len(postxt)),np.zeros(len(negtxt))),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'in', 'the', 'back', 'of', 'the', 'van', '!']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(data[10])\n",
    "print(label[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.1, random_state=1 ,stratify=label)\n",
    "X_train,X_dev,y_train,y_dev = train_test_split(X_train , y_train , test_size=0.1, random_state = 1 , stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laughton', 'enjoyable', 'in', 'this', 'low', 'budget', 'pirate', 'saga']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to Pytorch tensors\n",
    "\n",
    "We will first convert data to Pytorch tensors so they can be used in a neural network.\n",
    "To do that, you must first create a dictionnary that will map words to integers.\n",
    "Add to the dictionnary only words that are in the training set (be sure to understand why we do that!).\n",
    "\n",
    "Then, you can convert the data to tensors:\n",
    "- use tensors of longs: both the sentence and the label will be represented as integers, not floats!\n",
    "- these tensors do not require a gradient\n",
    "\n",
    "A tensor representing a sentence is composed of the integer representation of each word, e.g. [10, 256, 3, 4].\n",
    "Note that some words in the dev and test sets may not be in the dictionnary! (i.e. unknown words)\n",
    "You can just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict:\n",
    "    def __init__(self, words, unk=None):\n",
    "        self._unk = unk\n",
    "        self._word_to_id = dict()\n",
    "        self._id_to_word = list()\n",
    "\n",
    "        if unk in words:\n",
    "            raise RuntimeError(\"UNK word exists in vocabulary\")\n",
    "\n",
    "        if unk is not None:\n",
    "            self.unk_index = self._add_word(unk)\n",
    "\n",
    "        for word in words:\n",
    "            self._add_word(word)\n",
    "\n",
    "    # for internal use only!\n",
    "    def _add_word(self, word):\n",
    "        if word not in self._word_to_id:\n",
    "            id = len(self._id_to_word)\n",
    "            self._word_to_id[word] = id\n",
    "            self._id_to_word.append(word)\n",
    "            return id\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def str_to_id(self, word):\n",
    "        if self._unk is not None:\n",
    "            return self._word_to_id.get(word, self.unk_index)\n",
    "        else:\n",
    "            return self._word_to_id[word]\n",
    "\n",
    "    def id_to_str(self, id):\n",
    "        return self._id_to_word[id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def has_unk(self):\n",
    "        return self._unk is not None\n",
    "    \n",
    "    def unk(self):\n",
    "        return self.unk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict where each word is associated to a number\n",
    "token_index = {}\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "#Don't forget to add the unk = True because some words maybe in the dev or test but not in the train\n",
    "token_index = Dict(token_index, unk=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dict' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-c9403bc59456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Dict' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(token_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network definition\n",
    "\n",
    "The neural network should be defined as follows:\n",
    "- it takes as input a tensor that is a sequence of integers indexing word embeddings\n",
    "- it retrieves the word embeddings from an embedding table\n",
    "- it constructs the \"input\" of the MLP by summing over all embeddingsÂ (i.e. bag-of-word model)\n",
    "- it build a hidden represention using a MLP (1 layer? 2 layers? experiment! but maybe first try wihout any hidden layer...)\n",
    "- it project the hidden representation to the output space: it is a binary classification task, so the output space is a scalar where a negative (resp. positive) value means the review is negative (resp. positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size ,list_size, embedding_dim=10):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        # To create an embedding table: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        self.emb_db = torch.nn.Embedding(num_embeddings = vocab_size ,embedding_dim=embedding_dim)\n",
    "    \n",
    "        self.convs = torch.nn.ModuleList([torch.nn.Linear(size*embedding_dim,2) for size in list_size])\n",
    "                   \n",
    "        for layer in self.convs:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight.data)\n",
    "            torch.nn.init.zeros_(layer.bias.data)\n",
    "            \n",
    "        self.list_size = list_size\n",
    "        \n",
    "        \n",
    "        self.output = torch.nn.Linear(len(list_size), 1)\n",
    "        torch.nn.init.xavier_uniform_(self.output.weight.data)\n",
    "        torch.nn.init.zeros_(self.output.bias.data)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "       \n",
    "        \n",
    "        output_ = []       \n",
    "        for layer,size in zip(self.convs,self.list_size):\n",
    "            \n",
    "            #padding\n",
    "            if (inputs.shape[0] < size):\n",
    "                pad = torch.LongTensor([0 for i in range(size-inputs.shape[0])])\n",
    "                inputs = torch.cat((inputs,pad),0)\n",
    "            \n",
    "            x = self.emb_db(inputs)\n",
    "            output = []\n",
    "            \n",
    "        \n",
    "            for i in range(1, x.shape[0]):\n",
    "                conv_1 = torch.cat((x[i] ,x[i-1]),0)\n",
    "                output.append(layer(conv_1))\n",
    "           \n",
    "        \n",
    "            h= torch.Tensor(x.shape[0], x.shape[1]*2)\n",
    "            \n",
    "            \n",
    "            #print(output.shape)\n",
    "            h = torch.stack(output)\n",
    "            \n",
    "            h,_ = torch.max(h,0)\n",
    "#             output_.append(output)\n",
    "            \n",
    "#         print(output_)\n",
    "#         output_ = torch.tensor(output_)\n",
    "        print(h.shape)\n",
    "        h = h.reshape(self.list_size, len(h))\n",
    "        output = F.sigmoid(self.output(h))\n",
    "        return (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Create a loss function builder.\n",
    "\n",
    "- Pytorch loss functions are documented here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "- In our case, we are interested in *BCELoss* and *BCEWithLogitsLoss*. Read their documentation and choose the one that fits with your network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Write your training loop!\n",
    "\n",
    "- parameterizable number of epochs\n",
    "- at each epoch, print the mean loss and the dev accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 2], m2: [1 x 1] at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-5fe214fbb98b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlist_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-218-b62c30bc4a5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr, X_train, y_train, X_dev, y_dev, optimizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;31m#print(label.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-267-1c0552f1787f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 2], m2: [1 x 1] at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "model1 = CNN_classifier(vocab_size=len(token_index)+1 , list_size = [2])\n",
    "train(model1 , 30 , lr=0.1 , X_train =X_train ,y_train = y_train , X_dev = X_dev , y_dev = y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , epochs, lr ,X_train ,y_train , X_dev , y_dev , optimizer =\"SGD\"):\n",
    "    \n",
    "    #defining various types of optimizer\n",
    "    if (optimizer==\"SGD\"):\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr =lr, weight_decay=1e-4)   \n",
    "    if (optimizer== \"AdaDelta\"):\n",
    "        optimizer = torch.optim.Adadelta(params=model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "        \n",
    "        \n",
    "    #variables to see the progression\n",
    "    mean_loss_train = []\n",
    "    mean_loss_dev = []\n",
    "    acc_train = []\n",
    "    acc_dev = []\n",
    "    \n",
    "    \n",
    "    #passe le modele en mode training\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        a = 0\n",
    "        #defining parameters to print the evolution of the NN\n",
    "        accuracy = 0\n",
    "        mean_loss = 0\n",
    "        \n",
    "        #shuffle to reduce overfitting\n",
    "#         c = list(zip(X_train, y_train))\n",
    "#         random.shuffle(c)\n",
    "#         X_train, y_train = zip(*c)\n",
    "        \n",
    "        for sent,label in zip(X_train , y_train):\n",
    "#             print('LEN', len(sent))\n",
    "#             if (len(sent)<3):\n",
    "#                  a+=1\n",
    "#                  continue\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #creating the tensors for inputs\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "\n",
    "            label = torch.Tensor([label])   \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "            #print(label.size())\n",
    "            output = loss(pred , label)\n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            #backward and updates\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        mean_loss_train.append(mean_loss/len(X_train))\n",
    "        acc_train.append(accuracy/len(X_train))\n",
    "        print(\"DDDDDDDDDDDDDDDD\", a)\n",
    "        print(\"EPOCHS\" , str(i+1) ,\"On training : Mean Loss :\" , mean_loss/len(X_train) , \"Accuracy :\" ,accuracy/len(X_train))\n",
    "\n",
    "        \n",
    "        mean_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        ###testing on dev data#########\n",
    "        for sent,label in zip(X_dev,y_dev):\n",
    "            if (len(sent)<3):\n",
    "                continue\n",
    "            data = torch.LongTensor([token_index.str_to_id(word) for word in sent])\n",
    "            label = torch.Tensor([label])\n",
    "            \n",
    "            #prediction and loss\n",
    "            pred = model(data)\n",
    "            output = loss(pred , label)\n",
    "            \n",
    "            mean_loss+=output.item()\n",
    "            \n",
    "            \n",
    "            #computing accuracy\n",
    "            if (pred >= 0.5 and label==1):\n",
    "                accuracy += 1\n",
    "            if(pred<0.5 and label==0):\n",
    "                accuracy += 1\n",
    "            \n",
    "        print(\"EPOCH\" , str(i+1) ,\"On Dev  :Mean Loss :\" , mean_loss/len(X_dev) , \"Accuracy :\" ,accuracy/len(X_dev))\n",
    "        print()\n",
    "\n",
    "        mean_loss_dev.append(mean_loss/len(X_dev))\n",
    "        acc_dev.append(accuracy/len(X_dev))\n",
    "          \n",
    "            \n",
    "            \n",
    "    #Plotting loss accuracy for  test and train\n",
    "    plt.plot(acc_train , label='Train')\n",
    "    plt.plot(acc_dev, label=\"Dev\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(mean_loss_train , label='Train')\n",
    "    plt.plot(mean_loss_dev, label=\"Dev\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.figure()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
