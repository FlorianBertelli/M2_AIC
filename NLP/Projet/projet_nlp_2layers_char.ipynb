{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "projet_nlp_2layers_char.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SyiPsTOlsVwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9eac1f46-b818-49bf-9eda-f610a88e534b"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "2xnxuLlgsVwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4879282d-151f-4b52-ab03-f106010dc8bb"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import os\n",
        "# load ascii text and covert to lowercase\n",
        "\n",
        "full_text = []\n",
        "\n",
        "for filename in os.listdir(os.getcwd()):\n",
        "    if (filename!='sample_data' and filename!='.config'):\n",
        "        raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "        full_text.append(raw_text.lower())\n",
        "flat_list = (', '.join(full_text))\n",
        "print(len(flat_list))\n",
        "flat_list = flat_list[:100000].rstrip()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "644636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "ZhqcvstEsVwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ.?!')\n",
        "answer = ''.join(filter(whitelist.__contains__, flat_list))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K1-pT9f9sVwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(answer)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v-nrIiYNsVwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0c697dcd-cf8f-4129-800a-4c63a2bd7ae1"
      },
      "source": [
        "print(char_to_int)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '.': 2, '?': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PNDRHkeAsVwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "2aaf03b8-bced-4eb7-9f80-53c8feba5d67"
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(answer)\n",
        "n_vocab = len(chars)\n",
        "print( \"Total Characters: \", n_chars)\n",
        "print( \"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = answer[i:i + seq_length]\n",
        "\tseq_out = answer[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"../weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  96210\n",
            "Total Vocab:  30\n",
            "Total Patterns:  96110\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qw0uLNvhsVwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae51e27e-3190-4658-804d-465a93b3b6a1"
      },
      "source": [
        "model.fit(X, y, epochs=40, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "96110/96110 [==============================] - 308s 3ms/step - loss: 2.8794\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87937, saving model to ../weights-improvement-01-2.8794-bigger.hdf5\n",
            "Epoch 2/40\n",
            "96110/96110 [==============================] - 302s 3ms/step - loss: 2.6553\n",
            "\n",
            "Epoch 00002: loss improved from 2.87937 to 2.65534, saving model to ../weights-improvement-02-2.6553-bigger.hdf5\n",
            "Epoch 3/40\n",
            "96110/96110 [==============================] - 302s 3ms/step - loss: 2.5083\n",
            "\n",
            "Epoch 00003: loss improved from 2.65534 to 2.50833, saving model to ../weights-improvement-03-2.5083-bigger.hdf5\n",
            "Epoch 4/40\n",
            "96110/96110 [==============================] - 305s 3ms/step - loss: 2.3926\n",
            "\n",
            "Epoch 00004: loss improved from 2.50833 to 2.39258, saving model to ../weights-improvement-04-2.3926-bigger.hdf5\n",
            "Epoch 5/40\n",
            "96110/96110 [==============================] - 303s 3ms/step - loss: 2.2983\n",
            "\n",
            "Epoch 00005: loss improved from 2.39258 to 2.29832, saving model to ../weights-improvement-05-2.2983-bigger.hdf5\n",
            "Epoch 6/40\n",
            "96110/96110 [==============================] - 304s 3ms/step - loss: 2.2178\n",
            "\n",
            "Epoch 00006: loss improved from 2.29832 to 2.21781, saving model to ../weights-improvement-06-2.2178-bigger.hdf5\n",
            "Epoch 7/40\n",
            "96110/96110 [==============================] - 305s 3ms/step - loss: 2.1500\n",
            "\n",
            "Epoch 00007: loss improved from 2.21781 to 2.14999, saving model to ../weights-improvement-07-2.1500-bigger.hdf5\n",
            "Epoch 8/40\n",
            "96110/96110 [==============================] - 303s 3ms/step - loss: 2.0872\n",
            "\n",
            "Epoch 00008: loss improved from 2.14999 to 2.08720, saving model to ../weights-improvement-08-2.0872-bigger.hdf5\n",
            "Epoch 9/40\n",
            "96110/96110 [==============================] - 302s 3ms/step - loss: 2.0333\n",
            "\n",
            "Epoch 00009: loss improved from 2.08720 to 2.03335, saving model to ../weights-improvement-09-2.0333-bigger.hdf5\n",
            "Epoch 10/40\n",
            "96110/96110 [==============================] - 303s 3ms/step - loss: 1.9837\n",
            "\n",
            "Epoch 00010: loss improved from 2.03335 to 1.98370, saving model to ../weights-improvement-10-1.9837-bigger.hdf5\n",
            "Epoch 11/40\n",
            "96110/96110 [==============================] - 305s 3ms/step - loss: 1.9398\n",
            "\n",
            "Epoch 00011: loss improved from 1.98370 to 1.93982, saving model to ../weights-improvement-11-1.9398-bigger.hdf5\n",
            "Epoch 12/40\n",
            "96110/96110 [==============================] - 305s 3ms/step - loss: 1.8926\n",
            "\n",
            "Epoch 00012: loss improved from 1.93982 to 1.89257, saving model to ../weights-improvement-12-1.8926-bigger.hdf5\n",
            "Epoch 13/40\n",
            "96110/96110 [==============================] - 298s 3ms/step - loss: 1.8520\n",
            "\n",
            "Epoch 00013: loss improved from 1.89257 to 1.85203, saving model to ../weights-improvement-13-1.8520-bigger.hdf5\n",
            "Epoch 14/40\n",
            "96110/96110 [==============================] - 292s 3ms/step - loss: 1.8164\n",
            "\n",
            "Epoch 00014: loss improved from 1.85203 to 1.81638, saving model to ../weights-improvement-14-1.8164-bigger.hdf5\n",
            "Epoch 15/40\n",
            "96110/96110 [==============================] - 289s 3ms/step - loss: 1.7792\n",
            "\n",
            "Epoch 00015: loss improved from 1.81638 to 1.77924, saving model to ../weights-improvement-15-1.7792-bigger.hdf5\n",
            "Epoch 16/40\n",
            "96110/96110 [==============================] - 293s 3ms/step - loss: 1.7408\n",
            "\n",
            "Epoch 00016: loss improved from 1.77924 to 1.74084, saving model to ../weights-improvement-16-1.7408-bigger.hdf5\n",
            "Epoch 17/40\n",
            "96110/96110 [==============================] - 293s 3ms/step - loss: 1.7155\n",
            "\n",
            "Epoch 00017: loss improved from 1.74084 to 1.71554, saving model to ../weights-improvement-17-1.7155-bigger.hdf5\n",
            "Epoch 18/40\n",
            "96110/96110 [==============================] - 293s 3ms/step - loss: 1.6811\n",
            "\n",
            "Epoch 00018: loss improved from 1.71554 to 1.68108, saving model to ../weights-improvement-18-1.6811-bigger.hdf5\n",
            "Epoch 19/40\n",
            "96110/96110 [==============================] - 292s 3ms/step - loss: 1.6485\n",
            "\n",
            "Epoch 00019: loss improved from 1.68108 to 1.64847, saving model to ../weights-improvement-19-1.6485-bigger.hdf5\n",
            "Epoch 20/40\n",
            "96110/96110 [==============================] - 298s 3ms/step - loss: 1.6207\n",
            "\n",
            "Epoch 00020: loss improved from 1.64847 to 1.62067, saving model to ../weights-improvement-20-1.6207-bigger.hdf5\n",
            "Epoch 21/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.5972\n",
            "\n",
            "Epoch 00021: loss improved from 1.62067 to 1.59719, saving model to ../weights-improvement-21-1.5972-bigger.hdf5\n",
            "Epoch 22/40\n",
            "96110/96110 [==============================] - 293s 3ms/step - loss: 1.5693\n",
            "\n",
            "Epoch 00022: loss improved from 1.59719 to 1.56933, saving model to ../weights-improvement-22-1.5693-bigger.hdf5\n",
            "Epoch 23/40\n",
            "96110/96110 [==============================] - 292s 3ms/step - loss: 1.5488\n",
            "\n",
            "Epoch 00023: loss improved from 1.56933 to 1.54884, saving model to ../weights-improvement-23-1.5488-bigger.hdf5\n",
            "Epoch 24/40\n",
            "96110/96110 [==============================] - 293s 3ms/step - loss: 1.5222\n",
            "\n",
            "Epoch 00024: loss improved from 1.54884 to 1.52220, saving model to ../weights-improvement-24-1.5222-bigger.hdf5\n",
            "Epoch 25/40\n",
            "96110/96110 [==============================] - 295s 3ms/step - loss: 1.5040\n",
            "\n",
            "Epoch 00025: loss improved from 1.52220 to 1.50397, saving model to ../weights-improvement-25-1.5040-bigger.hdf5\n",
            "Epoch 26/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.4804\n",
            "\n",
            "Epoch 00026: loss improved from 1.50397 to 1.48044, saving model to ../weights-improvement-26-1.4804-bigger.hdf5\n",
            "Epoch 27/40\n",
            "96110/96110 [==============================] - 292s 3ms/step - loss: 1.4635\n",
            "\n",
            "Epoch 00027: loss improved from 1.48044 to 1.46354, saving model to ../weights-improvement-27-1.4635-bigger.hdf5\n",
            "Epoch 28/40\n",
            "96110/96110 [==============================] - 296s 3ms/step - loss: 1.4500\n",
            "\n",
            "Epoch 00028: loss improved from 1.46354 to 1.44996, saving model to ../weights-improvement-28-1.4500-bigger.hdf5\n",
            "Epoch 29/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.4255\n",
            "\n",
            "Epoch 00029: loss improved from 1.44996 to 1.42553, saving model to ../weights-improvement-29-1.4255-bigger.hdf5\n",
            "Epoch 30/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.4129\n",
            "\n",
            "Epoch 00030: loss improved from 1.42553 to 1.41287, saving model to ../weights-improvement-30-1.4129-bigger.hdf5\n",
            "Epoch 31/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.4004\n",
            "\n",
            "Epoch 00031: loss improved from 1.41287 to 1.40038, saving model to ../weights-improvement-31-1.4004-bigger.hdf5\n",
            "Epoch 32/40\n",
            "96110/96110 [==============================] - 295s 3ms/step - loss: 1.3862\n",
            "\n",
            "Epoch 00032: loss improved from 1.40038 to 1.38618, saving model to ../weights-improvement-32-1.3862-bigger.hdf5\n",
            "Epoch 33/40\n",
            "96110/96110 [==============================] - 294s 3ms/step - loss: 1.3690\n",
            "\n",
            "Epoch 00033: loss improved from 1.38618 to 1.36895, saving model to ../weights-improvement-33-1.3690-bigger.hdf5\n",
            "Epoch 34/40\n",
            "96110/96110 [==============================] - 297s 3ms/step - loss: 1.3570\n",
            "\n",
            "Epoch 00034: loss improved from 1.36895 to 1.35696, saving model to ../weights-improvement-34-1.3570-bigger.hdf5\n",
            "Epoch 35/40\n",
            "96110/96110 [==============================] - 295s 3ms/step - loss: 1.3488\n",
            "\n",
            "Epoch 00035: loss improved from 1.35696 to 1.34880, saving model to ../weights-improvement-35-1.3488-bigger.hdf5\n",
            "Epoch 36/40\n",
            "96110/96110 [==============================] - 306s 3ms/step - loss: 1.3337\n",
            "\n",
            "Epoch 00036: loss improved from 1.34880 to 1.33370, saving model to ../weights-improvement-36-1.3337-bigger.hdf5\n",
            "Epoch 37/40\n",
            "96110/96110 [==============================] - 312s 3ms/step - loss: 1.3266\n",
            "\n",
            "Epoch 00037: loss improved from 1.33370 to 1.32660, saving model to ../weights-improvement-37-1.3266-bigger.hdf5\n",
            "Epoch 38/40\n",
            "96110/96110 [==============================] - 313s 3ms/step - loss: 1.3161\n",
            "\n",
            "Epoch 00038: loss improved from 1.32660 to 1.31613, saving model to ../weights-improvement-38-1.3161-bigger.hdf5\n",
            "Epoch 39/40\n",
            "96110/96110 [==============================] - 311s 3ms/step - loss: 1.3045\n",
            "\n",
            "Epoch 00039: loss improved from 1.31613 to 1.30447, saving model to ../weights-improvement-39-1.3045-bigger.hdf5\n",
            "Epoch 40/40\n",
            "96110/96110 [==============================] - 316s 3ms/step - loss: 1.2952\n",
            "\n",
            "Epoch 00040: loss improved from 1.30447 to 1.29522, saving model to ../weights-improvement-40-1.2952-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6853c52b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "z7XceFQzsVwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "f9b36611-ea56-403a-9b19-cf35fcea8d09"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "...\n",
        "for i in range(10): \n",
        "    # pick a random seed\n",
        "    #start = numpy.random.randint(0, len(dataX)-1)\n",
        "    pattern = dataX[i*100-1]\n",
        "    print (\"Seed:\")\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "    # generate characters\n",
        "    for i in range(100):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print (\"\\nDone.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" used to shoot reagan. but not anymore. remember? listen stan. i know you love reagan. but you screwe \"\n",
            "d it for a good care.  she couser on scerol be a seconl  the move is someshing in the past cut i hav\n",
            "Done.\n",
            "Seed:\n",
            "\" . here at school you call me coach dipwad! where did those dogs come from? you ditching gym class to \"\n",
            " iild of to a boob.  you gave this carver to sell me she starte.  stan smiths mnd for my cupband soa\n",
            "Done.\n",
            "Seed:\n",
            "\" o? today is track. i hate running. then were on the same page. welcome to my sanctuary. look at them \"\n",
            " a puer to iilljal first siing to the stattes wer wo thin. i have a good cack to the stantaliy grom \n",
            "Done.\n",
            "Seed:\n",
            "\"  scurrying around like ants. go ahead exercise all you want youll never escape the smoky death of ti \"\n",
            "me. i have a teally bean of chance to me.  what are you doing to the mast couerser tiing on the stre\n",
            "Done.\n",
            "Seed:\n",
            "\" mes magnifying glass. wow thats a little dark. not at all. life is a banquet.  and death is dessert. \"\n",
            "  i cont know if it so me. the puestion is tiinks of coriusent and the corlseient seache for a bean.\n",
            "Done.\n",
            "Seed:\n",
            "\"   i love dessert! mmm. thats one impressive man. handsome fit the whole package. thats great stan. c \"\n",
            "ut what are you doing hare for the sapesrar. cont wourg the carter that gorsg hav thin to my mecker \n",
            "Done.\n",
            "Seed:\n",
            "\" an we stop staring at your reflection in the tv and actually turn it on now? i promised my mr belved \"\n",
            "ere fot the sas our nation was horesent to she starte. i dont know if it isst a cuss! roger we dan s\n",
            "Done.\n",
            "Seed:\n",
            "\" ere chat group id post a summary of this episode by . those losers are pretty punctual. good news ev \"\n",
            "eryone! im nnt in the soo.  what are you doing to the mast couer of the street recratoor offhth as i\n",
            "Done.\n",
            "Seed:\n",
            "\" eryone. im in love.  tell us about her sweetie.  her name is debbie. she smells like a glue stick sh \"\n",
            "at stert. i cont know if it isst a cuss! roger we dan sell you that way to tie starte. im not in the\n",
            "Done.\n",
            "Seed:\n",
            "\" e shares my interest in bug zappers and she likes reading old books by guys who died of syphilis. sy \"\n",
            "an you dont have to steve a good cack to the same mest fonla de boger. you know what i do i kike tha\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}