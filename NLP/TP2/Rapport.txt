Rapport TP2 NLP Bertelli Florian

Dans ce TP, on a d'abord tester wapiti avec le fichier de pattern de base, qui tient seulement compte 
de la colonne 0 des mots au dessus et en dessous et deux  bigrammes basiques.
On obtient les resultats suivants:

* Load model
* Label sequences
      1000 sequences labeled	 8.29%/46.20%
      2000 sequences labeled	 6.76%/47.45%
      3000 sequences labeled	 6.35%/43.63%
    Nb sequences  : 3684
    Token error   :  6.61%
    Sequence error: 42.37%
* Per label statistics
    O       Pr=0.95  Rc=0.99  F1=0.97
    I-ORG   Pr=0.82  Rc=0.62  F1=0.70
    I-MISC  Pr=0.82  Rc=0.67  F1=0.74
    I-PER   Pr=0.88  Rc=0.70  F1=0.78
    I-LOC   Pr=0.89  Rc=0.70  F1=0.79
    B-LOC   Pr=0.00  Rc=0.00  F1=-nan
    B-MISC  Pr=0.00  Rc=0.00  F1=-nan
    B-ORG   Pr=-nan  Rc=0.00  F1=-nan
* Done

Ensuite on a cherché à améliorer la modèle en ajoutant des patterns comme ci dessous:

Rapport TP2 NLP Bertelli Florian

Dans ce TP, on a d'abord tester wapiti avec le fichier de pattern de base, qui tiens seulement compte 
de la colonne 0 des mots au dessus et en dessous et deux  bigrammes basiques.
On obtient les resultats suivants:

* Load model
* Label sequences
      1000 sequences labeled	 8.29%/46.20%
      2000 sequences labeled	 6.76%/47.45%
      3000 sequences labeled	 6.35%/43.63%
    Nb sequences  : 3684
    Token error   :  6.61%
    Sequence error: 42.37%
* Per label statistics
    O       Pr=0.95  Rc=0.99  F1=0.97
    I-ORG   Pr=0.82  Rc=0.62  F1=0.70
    I-MISC  Pr=0.82  Rc=0.67  F1=0.74
    I-PER   Pr=0.88  Rc=0.70  F1=0.78
    I-LOC   Pr=0.89  Rc=0.70  F1=0.79
    B-LOC   Pr=0.00  Rc=0.00  F1=-nan
    B-MISC  Pr=0.00  Rc=0.00  F1=-nan
    B-ORG   Pr=-nan  Rc=0.00  F1=-nan
* Done

Ensuite on a cherché à améliorer la modèle en ajoutant des patterns comme ci dessous:

# Unigram
u1:%x[-2,0]
u2:%x[-1,0]
u3:%x[ 0,0]
u4:%x[ 1,0]
u5:%x[ 2,0]

#NewPatern
u6:%x[-2,1]
u7:%x[-1,1]
u8:%x[ 0,1]
u9:%x[ 1,1]
u10:%x[ 2,1]
u12:%x[-2,2]
u13:%x[-1,2]
u14:%x[ 0,2]
u15:%x[ 1,2]
u16:%x[ 2,2]


# Bigram
u17:%x[-1,0]/%x[0,0]
u18:%x[ 1,0]/%x[0,0]



Grâce à cela on a obtenu de meilleurs résultats, ce qui parait logique puisque
notre modèle va pouvoir apprendre sur plus de features. Voici les résultats obtenus:
* Load model
* Label sequences
      1000 sequences labeled	 5.99%/37.50%
      2000 sequences labeled	 5.24%/41.20%
      3000 sequences labeled	 4.92%/37.97%
    Nb sequences  : 3684
    Token error   :  5.01%
    Sequence error: 36.18%
* Per label statistics
    O       Pr=0.98  Rc=0.99  F1=0.98
    I-ORG   Pr=0.73  Rc=0.72  F1=0.72
    I-MISC  Pr=0.82  Rc=0.68  F1=0.74
    I-PER   Pr=0.81  Rc=0.88  F1=0.85
    I-LOC   Pr=0.85  Rc=0.75  F1=0.80
    B-LOC   Pr=-nan  Rc=0.00  F1=-nan
    B-MISC  Pr=-nan  Rc=0.00  F1=-nan
    B-ORG   Pr=-nan  Rc=0.00  F1=-nan
* Done




On a aussi rajouté une expression régulière, reconnaissant les majuscules, ce qui nous a aussi
améliorer notre modèle car le noms propres contiennent une majuscule en début de mot. Toutefois certains
mot, qui ne sont pas des entités nommées contiennent aussi une majuscule en début de mot.

#Expression reguliere
u11:%t[0,0,"^\u"]



On a aussi obtenu de meilleurs résultats : 



On passe de 36% à 32% d'erreur donc notre modèle s'est amélioré grâce a cette
expression régulière.

* Load model
* Label sequences
      1000 sequences labeled	 5.10%/33.10%
      2000 sequences labeled	 4.77%/37.35%
      3000 sequences labeled	 4.48%/34.00%
    Nb sequences  : 3684
    Token error   :  4.54%
    Sequence error: 32.46%
* Per label statistics
    O       Pr=0.99  Rc=0.99  F1=0.99
    I-ORG   Pr=0.76  Rc=0.75  F1=0.75
    I-MISC  Pr=0.78  Rc=0.73  F1=0.75
    I-PER   Pr=0.81  Rc=0.90  F1=0.85
    I-LOC   Pr=0.84  Rc=0.77  F1=0.80
    B-LOC   Pr=-nan  Rc=0.00  F1=-nan
    B-MISC  Pr=-nan  Rc=0.00  F1=-nan
    B-ORG   Pr=-nan  Rc=0.00  F1=-nan
* Done


Enfin, on a augmenté notre corpus grâce aux Pos-tagger de nltk, 

* Load model
* Label sequences
      1000 sequences labeled	 5.30%/32.30%
      2000 sequences labeled	 4.27%/33.15%
      3000 sequences labeled	 4.10%/32.00%
    Nb sequences  : 3684
    Token error   :  4.34%
    Sequence error: 32.96%
* Per label statistics
    O       Pr=0.99  Rc=0.99  F1=0.99
    I-ORG   Pr=0.76  Rc=0.76  F1=0.76
    I-MISC  Pr=0.78  Rc=0.73  F1=0.77
    I-PER   Pr=0.81  Rc=0.96  F1=0.86
    I-LOC   Pr=0.84  Rc=0.79  F1=0.82
    B-LOC   Pr=-nan  Rc=0.00  F1=-nan
    B-MISC  Pr=-nan  Rc=0.00  F1=-nan
    B-ORG   Pr=-nan  Rc=0.00  F1=-nan
* Done

On constate que nos résultats se sont légèrement améliorés, car 
les tags générés par ntlk pos tagger sont presque systématiquement
identique à celui fournit dans les fichiers train et test. 



Enfin, nous avons réalisé un dernier test avec l'algorithme de recognition
d'entité nommée de nltk via l'utilisation de la fonction nltk.ne_chunk. 
Nous avons eu des meilleurs résultats sur la séquence dans son ensemble avec
10,7% d'erreur sur le test. Toutefois l'implémentation de la fonction nltk.ne_chunk,
ne nous a pas permis de regarder plus précisement aux statistiques par label, car nous avons 
du coder nous même le mapping entre la valeur réelle et la valeur dévinée par ntlk. 

